---
title: Hadoop Deep Inside - Ch.3 하둡 분산 파일 시스템(HDFS)
categories: [Hadoop, Deep Inside]
---

이번 포스팅에서는 HDFS에 대한 내용을 본격적으로 다룹니다. 당연한 소리지만 HDFS가 GFS에서 시작된 만큼 초기 HDFS는 GFS의 원리, 특성 그리고 아키텍쳐까지도 상당부분 유사합니다. 용어가 조금씩 달라지는 부분이 있다는 정도로 이해하시면 편할 겁니다.

물론 이후의 하둡이 버전업되면서 독자적인 노선을 밟게 됩니다. 특히 2.0 버전부터 Namenode의 이중화, yarn 도입과 같은 큰 변화들이 있었습니다. 이러한 점에서 HDFS에 대한 포스팅은 크게 두 파트로 나누어 작성할 예정입니다. 첫째는, HDFS의 초기 모델에 대한 이야기 그리고 둘째는 하둡 2.0버전 이후의 HDFS에 대한 이야기입니다.

*(이 포스팅에서의 HDFS는 모두 초기의 HDFS를 의미합니다. 2.0이후의 버전을 다룰때는 별도의 포스팅을 통할 예정입니다.)*

# 1. GFS와 HDFS

GFS에 대해 공부한만큼, GFS와 HDFS를 간단히 비교하는 것으로 첫 단락을 시작해보겠습니다.

![GFS-architecture2](/images/GFS-architecture2.jpg)

대부분의 분산 시스템은 크게 두 가지 유형으로 나눌 수 있습니다. Master-Slave 구조와 Master가 없는 구조로 말이죠. 보시다시피 GFS는 대표적인 Master-Slave 구조의 분산 시스템입니다. GFS master가 이름 그대로 master이고 GFS chunkserver가 slave입니다.

일반적으로 Master-Slave구조의 시스템에서 slave는 다수로 구성이 되고 쉽게 확장가능합니다. 이를 ‘scale out이 용이하다’라고 말합니다. 이는 비단 하둡에 국한된 것이 아니라 대부분의 분산 플랫폼들이 slave의 scale out이 용이한 아키텍쳐를 가지고 있습니다.

실제로 GFS는 단일 master와 다수의 slave구조를 가지고 있습니다. 이러한 구조에서 중요한건 master에 부하가 가지 않는 상황을 만들어 주는 것입니다. 이는 마스터 쪽에 장애가 발생 시, 전체 클러스터의 장애로 이어지기 때문입니다. 이때 마스터를 단일 장애 지점(SPOF, Single Point Of Faillure)라고 부릅니다. (자세한 이야기는 하둡 2.0버전에 대해 다룰때 하겠습니다.)

위의 그림에서 굵은 화살표로 표현된 chunk data 부분을 보면, 실질적인 데이터를 주고받는 작업은 클라이언트와 chunkserver 사이에서만 일어난다는 것을 확인할 수 있습니다. 데이터를 주고받는 일은 트래픽이 많이 발생하기 때문에 이를 chunkserver에 전담시킴으로서 최대한 마스터에 부하가 안가도록 하기위한 설계한 것이죠.

![hdfs1](/images/hdfs1.jpg)

이번엔 HDFS를 보도록 하죠. HDFS도 Master-slave 구조를 가지고 있습니다. 앞서 말했듯 GFS와 상당히 유사한 형태죠. 다만, 일부 다른점도 존재합니다. 가령 GFS Master는 HDFS에서는 Name Node로 불리고 GFS의 Chunk Server는 Data Node라고 불립니다. 이뿐만아니라 GFS에서는 본적없던 Secondary namenode가 등장합니다.

![hdfs2](/images/hdfs2.jpg)

또한 HDFS도 GFS와 마찬가지로 단일 master와 다수의 slave구조를 가지고 있습니다. 이때문에 HDFS 역시 scale out이 용이하죠. 그리고 HDFS도 master의 부하를 줄이기위해 실제 데이터를 주고받는 작업은 Client와 Datanode 사이에서만 발생하록 설계되었습니다. 대신 namenode는 namespace에서 block(GFS에서의 chunk) 데이터들의 metadata를 보관합니다. 이 또한 GFS의 master와 같은 역할입니다.

# 2. HDFS란?

HDFS(Hadoop Distributed File System)는 세계에서 가장 신뢰할 수 있는 스토리지 시스템 중 하나입니다. HDFS는 하둡 클러스터에서 작동하는 매우 큰 파일들을 분산 저장하는 소프트웨어로써, 하드웨어가 장애가 생긴 경우에도 데이터를 저장할 수 있는 신뢰성 있는 시스템을 형성해주기 때문이죠.

여기서 HDFS는 소프트웨어라는 개념을 기억해둘 필요가 있습니다. 왜냐면 저처럼 CS지식이 부족한 사람이라면, 하둡 클러스터에서 HDFS를 이야기할 때 조금 헷갈릴 수 있기 때문이죠.

솔직히 초기 HDFS는 GFS와 크게 다를바가 없습니다. 너무나도 많은 부분들이 유사하죠. 뭐 당연한 이야기지만, HDFS가 GFS를 보고 만들었으니까요.

그래도 HDFS에 대해서 따로 공부해야하는 이유가 있습니다. 분명히 GFS와 다른 지점들도 존재하기 때문이죠. GFS를 토대로 만들었다고 해서 그걸 배끼기만 했다는 건 아니니까요.

따라서 이번 단락에서는 HDFS의 주요 요소들에 대해 하나씩 살펴볼 겁니다. 그리고 이를 통해 GFS와 어떤 점이 다른지 그리고 어떤 길로 나아가려한건지를 이해해 보도록 하겠습니다. 이를 하둡 1.0버전으로 구성된 클러스터를 설명하면서 시작해보도록 하겠습니다.

## 1) 하둡 클러스터 네트워크 및 데몬 구성

![hdfs3](/images/hdfs3.jpg)

위의 그림은 IDC(Internet Data Center)에 구축된 하둡 클러스터를 간단히 도식화한 것입니다. 이 IDC에서는 Rack형 클러스터로  하둡 클러스터를 구축했고 각 Rack에는 rack 스위치와 여러대의 서버들이 들어가 있습니다. 보통 기업들은 수백대의 서버로 하둡 클러스터를 구축하고 있습니다.

GFS에서의 가정을 기억한다면, 하둡 클러스터도 상용 하드웨어(commodity hardware)로 구성된 클러스터입니다. rack안에 들어있는 머신들은 각각 하나의 상용 하드웨어이고 서버인것 입니다. 그리고 하둡 클러스터는 master 역할을 하는 서버와 slave 역할을 하는 서버로 나누어 집니다.

여기서 하둡이 필요한 이유가 나옵니다. 만약 분산 저장 처리 환경을 구축하기 위해서 하둡이 없다면 어떨까요?

물리적으로 떨어져있는 모든 하드웨어들을 일일히 연결해주고 설정해야한다고 생각해보죠. 클러스터가 2~3개의 서버만 사용한다면 해볼만 할지도 모릅니다. 그러나 100개 300개 1000개까지 늘어난다고 생각하면, 이는 꽤나 끔찍합니다.

이를 쉽게 해주는게 하둡입니다. 하둡은 수많은 서버로 클러스터를 구성하는 일과 이들 사이의 연결, 그리고 역할을 부여하는 일을 하여 대용량 데이터의 분산 저장 및 처리 환경을 갖춰줍니다.

이때, 하둡 직접 자체가 분산 저장하고 분산 처리하는 것이 아니라는 걸 명심하셔야합니다. 하둡은 환경이 갖춰질수 있는 프레임워크를 제공하는 것 뿐이고 실제로는 하둡의 모듈들의 작동을 통해 분산 저장 및 처리가 이루어집니다. 즉, 분산 저장은 HDFS가하고 분산 처리는 MapReduce가 하기때문에 하둡을 정의할 때 소프트웨어 프레임워크라고 하게되는 것이죠. ( 하둡 2.0버전 이후에서는 MapReduce의 분산 처리역할이 Yarn으로 이전되긴 합니다)

하둡 클러스터의 master 서버에는 하둡의 모듈들에서 master역할을 하는 데몬(daemon)을 띄웁니다. 같은 맥락에서 slave 서버에서는 slave 역할을 하는 데몬을 띄우겠죠.

 위의 하둡 클러스터 그림에서 빨강색은 하둡 클러스터에서 master 역할을 하는 서버들을 의미합니다. name node, job tracker, secondary namenode가 이에 해당하죠. 일반적으로 마스터 서버에서는 마스터 역할을 하는 데몬 단 하나만 띄웁니다. 이는 오롯히 마스터 서버는 마스터 데몬만 실행시켜 안정성을 높히려는 이유에서 입니다. GFS에서 마스터는 항상 부하가 최소화되는 환경으로 구축하였던 것이 HDFS에도 동일하게 적용된 것이죠.

반면 파랑색은 하둡 클러스터의 slave역할을 하는 서버들입니다. 모두 공통적으로 datanode와 task tracker가 데몬으로 실행되어 있습니다. 마스터 서버와 달리 복수의 데몬을 실행시켜놓습니다. 슬래이브 서버는 다수로 이루어져있기 때문에 마스터 서버처럼 장애를 걱정하기 보단, scale out을 통한 이점을 누리는 것이 더 좋습니다. 그렇기 때문에 slave 역할을 하는 서버에서는 slave 역할을 하는 데몬들을 모두 실행시키는 것입니다.

말이 좀 길었던거 같으니 짧고 간단하게 정리보죠.

- 하둡 클러스터는 Rack 단위로 클러스터를 구축하는 것이 일반적이다.
- 분산 저장은 HDFS, 분산 처리는 MapReduce가 한다.
- 주요 데몬으로는 HDFS의 NameNode와 MapReduce의 JobTracker가 있다.
- 각각은 DataNode와 TaskTracker라는 슬레이브 데몬을 관리하는 마스터 데몬이다.
- 마스터 데몬은 마스터 서버에서, 슬레이브 데몬은 슬레이브 서버에서 실행된다.

## 2) HDFS의 구성요소

이번 단락에서는 HDFS의 구성요소와 주요 개념들에 대해 알아보겠습니다.

### (1) 블록(Block)

![hdfs4](/images/hdfs4.jpg)

블록(Block)은 HDFS에서 파일이 나눠지는 단위입니다. 좀 더 정확하게는 파일 시스템에 저장되는 가장 작은 단위의 데이터입니다. GFS의 청크(Chunk)와 같은 개념이죠. 따라서 블록사이즈의 디폴트 값도 청크사이즈와 같은 64MB입니다. (이후 하둡 2.0버전으로 업그레이드 되면서 128MB로 증가합니다.)

위의 그림을 보면 160MB짜리 파일이 64MB짜리 블록 2개와 32MB짜리 블록 1개로 나눠지는 것을 볼 수 있습니다. 여기서 우리가 알아야할 건 두가지입니다.

우선 첫째, 블록 사이즈의 설정값이 64MB라는 건 64MB이상의 파일데이터가 64MB의 블록들로 나눠진다는 것을 의미한다.

둘째, 64MB의 블록으로 나누고 남은 데이터는 남은 데이터 크기 만큼의 사이즈를 가진 블록으로 만들어진다.

여기서 뭔가 의아함이 느껴지지않나요?

분명 앞선 정의에서는 블록이 파일 시스템에 저장되는 ‘가장 작은 단위의 데이터’라고 했습니다. 근데 실제 만들어진 블록 중에 64MB보다 작은 블록이 있습니다. 뭔가 블록의 정의에 모순되는 블록이 만들어진듯하죠.

이러한 현상의 비밀엔 블록이 ‘단위’라는 것에 있습니다. 블록이란건 64MB의 데이터 조각이라는 일차원적인 개념이 아니라, 파일 데이터가 나눠지는 단위 그 자체를 블록이라고 이해해야한다는 겁니다.

다시 풀어 말해보죠. 우리가 블록사이즈를 설정하는 건 블록사이즈의 상한선을 설정하는 개념입니다.

HDFS는 파일이 들어오면 블록이라는 단위로 나눈 후 분산해서 저장할 건데, 그 블록의 크기는 최대 64MB가 되게 하겠다는 거죠. 따라서 64MB씩 꽉꽉 눌러담아서 나눌건데, 만약 남은 파일 데이터의 크기가 64MB보다 작으면 거기에 맞는 블록을 가져와서 담겠다는 겁니다.

단순하게 예를 들어서 1KG짜리 쌀포대를 샀다고 해보죠. 그걸 이제 150g의 쌀을 담을 수 있는 플라스틱 병에 소분할 겁니다. 그러면 150g짜리 플라스틱병을 꽉채워서 6개 나오고, 남은 100g의 쌀은 100g을 담을 수 있는 플라스틱 병에 담습니다. 일부러 150g보다 적게 담거나 100g짜리 플라스틱병이 있는데 150g짜리 플라스틱병을 쓰거나 하진 않을겁니다. 왜냐면 그건 명백히 낭비니까요.

![hdfs5](/images/hdfs5.jpg)

이건 HDFS에서도 같은 맥락입니다. 64MB 블록을 일단 만들고 거기에 32MB만 담는다 하더라도 일단 그 블록사이즈가 64MB기 때문에 전체 디스크에서는 64MB만큼의 공간을 차지합니다. 따라서 이와 같이 불필요하게 공간의 낭비가 계속 일어나는 걸 막기 위해서 블록사이즈는 상한선 개념으로 작동하게 되는 것입니다.

사실 이 메커니즘은 GFS에서도 동일하게 작동합니다. 이 부분을 지난 GFS 챕터에서 설명하지 않았기 때문에 좀 길게 설명해 두었습니다.

추가적으로 블록사이즈는 청크사이즈와 마찬가지로 자유롭게 설정을 통해 결정할 수 있고, 그 크기는 최대 데이터노드 서버의 스토리지의 크기까지 가능합니다. 데이터노드 서버의 스토리지가 무한하게 크다면, 무한대가 되는것이죠.

### (1) 네임노드(NameNode)와 데이터노드(DataNode)
![hdfs6](/images/hdfs6.jpg)

HDFS에서 네임노드는는 마스터노드라고도 불립니다. master-slave 구조에서 master의 역할을 맡는 다는 것이죠. 그리고 HDFS에서 slave 역할을 맡는 건 데이터노드입니다.
