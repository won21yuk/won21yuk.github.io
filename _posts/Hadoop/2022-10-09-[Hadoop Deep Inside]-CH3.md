---
title: Hadoop Deep Inside - Ch.2 구글 파일 시스템(Google File System)
categories: [Hadoop, Deep Inside]
---

저는 앞선 챕터에서 분산 파일 시스템(DFS)라는 운영체제에 대한 공부를 했습니다. GFS와 HDFS의 근간이 되는 개념이죠.
그럼 또 한가지 의문이 생깁니다. HDFS는 GFS 논문을 보고 만들었고 GFS는 DFS 개념을 토대로 작성되었으니, 이제는 GFS 뭘까하는 의문이 말이죠.
이 의문은 너무나도 자연스러워서 거부할 수 없는 흐름을 만들어냈습니다. 그렇기에 전 이번 챕터에서 HDFS를 알기위한 그 두번째 여정으로 GFS가 무엇인지에 대해 알아보려합니다.
다소 너무 돌아가는 듯한 느낌이 들 수도 있지만, Deep Inside라는 취지에 맞춰서 끈기있게 진행해 보도록 하겠습니다.

# 1. Google File Systme(GFS) 개요

구글 파일 시스템(이하 GFS)의 등장에는 2000년 전후에 기하급수적으로 성장하기 시작한 인터넷 시장이 있습니다. 인터넷 시장이 커짐에 따라 구글의 데이터 처리 요구 사항이 급증했고, 구글은 해당 욕구를 충족하기 위해 GFS를 설계하고 구현한 것이죠.

GFS는 기본적으로 이전의 분산 파일 시스템(이하 DFS)과 같은 목표들을 많이 공유하고 있습니다. 성능, 신뢰성, 확장성, 가용성, 내고장성같은 목표들말이죠. 하지만 구글은 동시에 그들의 애플리케이션의 워크로드나 자사의 기술적 환경을 관찰하여 그 결과를 GFS 설계에 반영했습니다. 초기 DFS의 가정이 구글이 현재 처한 상황과 확연히 다르다고 여겼기 때문입니다. 이에 따라 구글은 DFS 기존 특징들에 대해 재검토하고 근본적으로 다른 점을 탐구했습니다.

## 첫째, 컴포넌트(부품) 고장은 예외라기보다는 일반적이다.

구글 서버의 많은 컴퓨터들은 비교적 싸고, 평범한 범용 부품들로 구축된 수백, 수천대의 스토리지 시스템으로 구성됩니다. 이는 컴퓨터 중 일부는 작동하지 않을 수 있고, 고장으로부터 회복되지 않을 가능성이 높다는 것을 시사합니다. 실제로 구글은 어플리케이션 에러, OS 에러, 사람이 촉발한 에러, 디스크/메모리/커넥터/네트워크 및 전원 공급장치 에러가 발생하는 것을 관찰했습니다.

구글은 이러한 컴포넌트들의 고장 및 장애를 예외적인 상황으로 간주하는 대신, 일반적인 상황으로 산정했습니다. 그래서 구글은 지속적인 모니터링, 에러 탐지, 내고장성 등이 필수적으로 시스템에 적용되어야 한다고 여겼습니다.

## 둘째, 파일은 전통적인 기준으로 봤을 때 거대하며, 다중 GB(기가바이트) 파일은 일반적이다.

인터넷 시장이 커져감에 따라 구글이 처리해야했던 파일의 크기 또한 비대해져갔습니다. 다중 GB파일이 일반적이다라는 말은 GB(기가바이트. 우리가 아는 그파일단위 맞음 kb < mb < gb)단위의 파일들이 많아졌다는 정도로 이해하면 됩니다. 특히나 구글같이 웹 기반의 거대 IT 기업은 관리해야 하는 파일에 웹 문서나  응용 프로그램 객체가 포함되어 있기 때문에 해당 파일의 용량이 큰 건 일반적인 것이라고 할 수 있습니다.

하지만 이러한 거대한 파일을 전통적인 DFS처럼 KB(키로바이트)단위로 나누어서 다루는 것은 매우 까다로운 일입니다. 구글이 정기적으로 처리해야하는 데이터의 크기가 GB를 넘어 TB단위였고, 파일 시스템이 이를 지원할 수 있다고 하더라도 수십억개에 달하는 KB단위의 파일을 관리한다는건 너무나도 어렵기 때문입니다. 따라서 구글은 거대한 파일을 효율적으로 다루기 위해선,  I/O 오퍼레이션이나 블록 사이즈같은 시스템의 전반적인 설계 가정이나 파라미터에 대한 재검토가 필요하다고 여겼습니다.

> **I/O Operation**
: 주변 장치와 주 메모리간에 데이터를 전송하고 중앙 처리 장치(CPU)가 연결된 주변 장치를 제어할 수 있도록 하는 일련의 I/O 작업(명령)
>

> **블록 사이즈**
: 파일을 어느정도의 단위로 쪼개서 분산 시킬 것인가에 대한 것
>

## 셋째, 대부분의 파일은 기존 데이터를 덮어쓰기보단 새로운 데이터를 추가하는 방식으로 변경된다.

구글은 대부분의 파일이 기존 데이터를 덮어쓰기(Overwrite)보단 새로운 데이터를 추가(Append)하는 방식으로 변경된다는 것을 관찰했습니다. 또한 파일 내에서 임의 쓰기는 사실상 존재하지 않으며, 한번 작성된 파일은 보통 읽기 전용이고 순차적으로 읽힌다는 것을 알았습니다. 그리고 데이터 스트림, 아카이브 데이터, 중간 결과물 형태의 데이터 등과 같은 다양한 데이터들이 이러한 특성을 공유한다는 것을 알게됐습니다.

이로써 구글은 이러한 파일 접근 방식이 대용량 파일에 적용했을 때를 가정했을 때, 기능 향상과 원자성 보장(Atomicity Guarantee)을 위해서 중점을 두어야 할 부분이 클라이언트의 데이터 블록 캐싱 알고리즘이 아니라 파일의 추가(append) 알고리즘이라고 결론지었습니다.

## 넷째, 어플리케이션과 파일 시스템 API를 공동 설계하면 유연성을 높여 전체 시스템이 도움이 된다.

예를 들어 GFS의 일관성 모델은 에플리케이션 쪽의 부담을 줄여주는 단순한 방식으로 구현되었으며, 클라이언트에서 동시적으로 하나의 파일에 append 하는 작업에서 클라이언트가 별도의 동기화 과정없이 가능하게 하기 위해 atomic append 오퍼레이션을 도입했습니다.

> **API(Application Programming Interface)**
: 애플리케이션에서 사용할 수 있도록 운영 체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스
>
